{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages for the project\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready the environment variables from the .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True) \n",
    "\n",
    "TOKENS = os.getenv(\"TOKENS\", \"\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9904f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for API usage\n",
    "\n",
    "API_URL = 'https://api.github.com'\n",
    "\n",
    "LIMIT_REQUESTS_BY_TOKEN_PER_HOUR = 5000\n",
    "LIMIT_REQUESTS_BY_IP_PER_HOUR = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1248d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API request utils (RUN IT ONLY ONCE TO KEEP COUNTER SYNCED)\n",
    "\n",
    "import requests\n",
    "\n",
    "# Initialize the counter for each token\n",
    "counter = {token: LIMIT_REQUESTS_BY_TOKEN_PER_HOUR for token in TOKENS}\n",
    "counter.update({None: LIMIT_REQUESTS_BY_IP_PER_HOUR})\n",
    "\n",
    "# Get the next avaliable token to use for the request\n",
    "def get_avaliable_token(): \n",
    "    token = None\n",
    "    for t in counter:\n",
    "        if counter[t] > 0:\n",
    "            token = t\n",
    "            break\n",
    "        else:\n",
    "            invalidate_token(t)\n",
    "    \n",
    "    return token\n",
    "    \n",
    "# Invalidate a token if it results in a error, avoiding to use it again\n",
    "def invalidate_token(token):\n",
    "    if token in counter:\n",
    "        del counter[token]\n",
    "        print(f\"Token {token} has been invalidated.\")\n",
    "    else:\n",
    "        print(f\"Token {token} not found in the list.\")\n",
    "\n",
    "# Mark a token as used, reducing the counter\n",
    "def used_token(token):\n",
    "    if token in counter:\n",
    "        counter[token] -= 1\n",
    "        # print(f\"Token {token} used, remaining requests: {counter[token]}\")\n",
    "        if counter[token] == 0:\n",
    "            invalidate_token(token)\n",
    "    else:\n",
    "        print(f\"Token {token} not found in the list.\")\n",
    "\n",
    "# Function to make a request to the API\n",
    "def request (path):\n",
    "    # Set the URL to the API endpoint\n",
    "    url = f'{API_URL}/{path}'\n",
    "    if (path.startswith(API_URL)):\n",
    "        url = path\n",
    "\n",
    "    # Define headers with the token\n",
    "    token = get_avaliable_token()\n",
    "    headers = {}\n",
    "    if token is not None:\n",
    "        headers['Authorization'] = f'Bearer {token}'\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.ok:\n",
    "        # print(f\"Request successful for token: {token}\")\n",
    "        used_token(token)\n",
    "    else:\n",
    "        if response.status_code == 401:\n",
    "            print(f\"Unauthorized access for token: {token}\")\n",
    "            used_token(token)\n",
    "\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"Rate limit exceeded for token: {token}\")\n",
    "            invalidate_token(token)\n",
    "            # Retry with the next available token\n",
    "            if (token is not None):\n",
    "                print(\"Retrying with the next available token...\")\n",
    "                return request(path)\n",
    "            \n",
    "        elif response.status_code == 404:\n",
    "            print(\"Resource not found\")\n",
    "\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limit exceeded for IP\")\n",
    "            invalidate_token(token)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and form/save initial graph\n",
    "\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "SEED_USER = 'gabriel-dp' # Initial node\n",
    "MAX_REQUESTS = 15000 # Max Requests (generally keep 5k per token)\n",
    "MAX_DEPTH = 3 # How deep will we search\n",
    "BLOCKED_USERS = {'torvalds'} # Can desconsider some users\n",
    "SLEEP_TIME = 0.2  # Delay between requests\n",
    "\n",
    "# Cache for using less requests\n",
    "followers_cache = {}\n",
    "following_cache = {}\n",
    "\n",
    "# Fetch followers/following (Can limit number of pages)\n",
    "def get_all_follow_data(username, follow_type='followers'):\n",
    "    print(f\"getting all follow data for {username}\")\n",
    "    if follow_type == 'followers' and username in followers_cache:\n",
    "        return followers_cache[username]\n",
    "    if follow_type == 'following' and username in following_cache:\n",
    "        return following_cache[username]\n",
    "\n",
    "    per_page = 100\n",
    "    page = 1\n",
    "    pages_limit = 10\n",
    "    results = []\n",
    "\n",
    "    while True:\n",
    "        path = f'users/{username}/{follow_type}?per_page={per_page}&page={page}'\n",
    "        response = request(path)\n",
    "        if not response or not response.ok:\n",
    "            break\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            break\n",
    "        results.extend([user['login'] for user in data])\n",
    "        if len(data) < per_page or page >= pages_limit:\n",
    "            break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "    if follow_type == 'followers':\n",
    "        followers_cache[username] = results\n",
    "    else:\n",
    "        following_cache[username] = results\n",
    "\n",
    "    return results\n",
    "\n",
    "# Build graph (mutual)\n",
    "def build_mutual_graph(seed_user, max_requests=10000, max_depth=3):\n",
    "    G = nx.Graph()\n",
    "    visited = set()\n",
    "    queue = [(seed_user, 0)]\n",
    "    total_requests = 0\n",
    "\n",
    "    while queue and total_requests < max_requests:\n",
    "        user, level = queue.pop(0)\n",
    "\n",
    "        if user in visited or user in BLOCKED_USERS or level > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {user} (level {level}) | Total requests: {total_requests}\")\n",
    "        visited.add(user)\n",
    "\n",
    "        try:\n",
    "            followers = get_all_follow_data(user, 'followers')\n",
    "            total_requests += 1\n",
    "            following = get_all_follow_data(user, 'following')\n",
    "            total_requests += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {user}: {e}\")\n",
    "            continue\n",
    "\n",
    "        mutuals = set(followers).intersection(following)\n",
    "        for mutual in mutuals:\n",
    "            if mutual in BLOCKED_USERS:\n",
    "                continue\n",
    "            G.add_edge(user, mutual)\n",
    "\n",
    "            if mutual not in visited:\n",
    "                queue.append((mutual, level + 1))\n",
    "\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "    print(f\"Finished: {len(G.nodes())} nodes, {len(G.edges())} edges, {total_requests} requests\")\n",
    "    return G\n",
    "\n",
    "# Save graph\n",
    "def save_graph(graph, filename='github_mutual_follow_graph.graphml'):\n",
    "    nx.write_graphml(graph, filename)\n",
    "    print(f\"Graph saved to {filename}\")\n",
    "\n",
    "# Entry piont\n",
    "if __name__ == \"__main__\":\n",
    "    graph = build_mutual_graph(SEED_USER, MAX_REQUESTS, MAX_DEPTH)\n",
    "    save_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dddab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and print the caracterization of the network\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load graph from file\n",
    "filename = 'github_mutual_follow_graph.graphml'\n",
    "G_original = nx.read_graphml(filename)\n",
    "mutual_edges = [(u, v) for u, v in G_original.edges() if G_original.has_edge(v, u)]\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(mutual_edges)\n",
    "\n",
    "print(\"Número de vértices:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())\n",
    "\n",
    "# Plot degree distribution\n",
    "degree_sequence = [d for n, d in G.degree()]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(degree_sequence, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribuição de graus\")\n",
    "plt.xlabel(\"Grau\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate clustering\n",
    "clustering = nx.clustering(G)\n",
    "avg_clustering = np.mean(list(clustering.values()))\n",
    "print(\"Coeficiente de clustering médio:\", avg_clustering)\n",
    "\n",
    "full_clustering_nodes = [n for n, c in clustering.items() if c == 1.0]\n",
    "print(\"Número de nós com clustering = 1.0:\", len(full_clustering_nodes))\n",
    "\n",
    "# Plotting degrees of clusterings of 1.0\n",
    "sizes = [G.degree(n) for n in full_clustering_nodes]\n",
    "print(\"Graus médios dos nós com clustering = 1.0:\", np.mean(sizes))\n",
    "plt.hist(sizes, bins=20, color='orange', edgecolor='black')\n",
    "plt.title(\"Distribuição de grau dos nós com clustering = 1.0\")\n",
    "plt.xlabel(\"Grau\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()\n",
    "\n",
    "# Calculates centralities (degree and eigenvector)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "try:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "except nx.PowerIterationFailedConvergence:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "print(\"eigenvector centrality done\")\n",
    "\n",
    "# Top 5 by degree centrality\n",
    "top_5_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 usuários por centralidade de grau:\")\n",
    "for node, centrality in top_5_degree:\n",
    "    print(f\"Nó: {node}, Centralidade de grau: {centrality:.4f}\")\n",
    "\n",
    "# Top 5 by eigenvector centrality\n",
    "top_5_eigen = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"\\nTop 5 usuários por centralidade de autovetor:\")\n",
    "for node, centrality in top_5_eigen:\n",
    "    print(f\"Nó: {node}, Centralidade de autovetor: {centrality:.4f}\")\n",
    "\n",
    "# Remove edges with lower than 1 degree for performance and better visualization\n",
    "low_degree_nodes = [n for n, d in G.degree() if d <= 5]\n",
    "G.remove_nodes_from(low_degree_nodes)\n",
    "print(\"Número de vértices:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())\n",
    "\n",
    "\n",
    "# Scaling node size\n",
    "scaler = MinMaxScaler(feature_range=(5, 150))\n",
    "deg_vals = np.array([degree_centrality[n] for n in G.nodes()]).reshape(-1, 1)\n",
    "eig_vals = np.array([eigenvector_centrality[n] for n in G.nodes()]).reshape(-1, 1)\n",
    "degree_sizes = scaler.fit_transform(deg_vals).flatten()\n",
    "eigen_sizes = scaler.fit_transform(eig_vals).flatten()\n",
    "print(\"scaling done\")\n",
    "\n",
    "# Layout (kamada_kawai or spring)\n",
    "pos = nx.spring_layout(G, k=1.5, iterations=200, seed=42)\n",
    "print(\"positioning done\")\n",
    "\n",
    "# Degree plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=degree_sizes, alpha=0.8, node_color='teal')\n",
    "print(\"drawn nodes\")\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.05, edge_color='gray', width=0.3)\n",
    "print(\"drawn edges\")\n",
    "plt.title(\"Tamanhos proporcionais à centralidade de grau\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Eigenvector plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=eigen_sizes, alpha=0.8, node_color='royalblue')\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.05, edge_color='gray', width=0.3)\n",
    "plt.title(\"Tamanhos proporcionais à centralidade de autovetor\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting subgraphs\n",
    "\n",
    "k = 300\n",
    "# Top k degree graph\n",
    "top_nodes_degree = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:k]\n",
    "G_top_degree = G.subgraph(top_nodes_degree)\n",
    "\n",
    "# Top K eigenvector graph\n",
    "top_nodes_eigenvector = sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)[:k]\n",
    "G_top_eigenvector = G.subgraph(top_nodes_eigenvector)\n",
    "\n",
    "def scale_sizes(subgraph, centrality_dict):\n",
    "    scaler = MinMaxScaler(feature_range=(5, 150))\n",
    "    centrality_vals = np.array([centrality_dict[n] for n in subgraph.nodes()]).reshape(-1, 1)\n",
    "    scaled_sizes = scaler.fit_transform(centrality_vals).flatten()\n",
    "    return scaled_sizes\n",
    "\n",
    "# degree plot\n",
    "degree_sizes_sub = scale_sizes(G_top_degree, degree_centrality)\n",
    "# pos_degree = nx.kamada_kawai_layout(G_top_degree)\n",
    "pos_degree = nx.spring_layout(G_top_degree, k=0.5, iterations=200, seed=42)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G_top_degree, pos_degree, node_size=degree_sizes_sub, alpha=0.8, node_color='teal')\n",
    "nx.draw_networkx_edges(G_top_degree, pos_degree, alpha=0.1, edge_color='gray', width=0.4)\n",
    "plt.title(f\"Top {k} Nós - Centralidade de Grau\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# eigenvector plot\n",
    "eigen_sizes_sub = scale_sizes(G_top_eigenvector, eigenvector_centrality)\n",
    "# pos_eigen = nx.kamada_kawai_layout(G_top_eigenvector)\n",
    "pos_eigen = nx.spring_layout(G_top_eigenvector, k=0.5, iterations=200, seed=42)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G_top_eigenvector, pos_eigen, node_size=eigen_sizes_sub, alpha=0.8, node_color='royalblue')\n",
    "nx.draw_networkx_edges(G_top_eigenvector, pos_eigen, alpha=0.1, edge_color='gray', width=0.4)\n",
    "plt.title(f\"Top {k} Nós - Centralidade de Autovetor\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
