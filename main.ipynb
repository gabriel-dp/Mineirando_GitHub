{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages for the project\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready the environment variables from the .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True) \n",
    "\n",
    "TOKENS = os.getenv(\"TOKENS\", \"\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9904f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for API usage\n",
    "\n",
    "API_URL = 'https://api.github.com'\n",
    "\n",
    "LIMIT_REQUESTS_BY_TOKEN_PER_HOUR = 100000\n",
    "LIMIT_REQUESTS_BY_IP_PER_HOUR = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1248d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API request utils (RUN IT ONLY ONCE TO KEEP COUNTER SYNCED)\n",
    "\n",
    "import requests\n",
    "\n",
    "# Initialize the counter for each token\n",
    "counter = {token: LIMIT_REQUESTS_BY_TOKEN_PER_HOUR for token in TOKENS}\n",
    "counter.update({None: LIMIT_REQUESTS_BY_IP_PER_HOUR})\n",
    "\n",
    "# Get the next avaliable token to use for the request\n",
    "def get_avaliable_token(): \n",
    "    token = None\n",
    "    for t in counter:\n",
    "        if counter[t] > 0:\n",
    "            token = t\n",
    "            break\n",
    "        else:\n",
    "            invalidate_token(t)\n",
    "    \n",
    "    return token\n",
    "    \n",
    "# Invalidate a token if it results in a error, avoiding to use it again\n",
    "def invalidate_token(token):\n",
    "    if token in counter:\n",
    "        del counter[token]\n",
    "        print(f\"Token {token} has been invalidated.\")\n",
    "    else:\n",
    "        print(f\"Token {token} not found in the list.\")\n",
    "\n",
    "# Mark a token as used, reducing the counter\n",
    "def used_token(token):\n",
    "    if token in counter:\n",
    "        counter[token] -= 1\n",
    "        # print(f\"Token {token} used, remaining requests: {counter[token]}\")\n",
    "        if counter[token] == 0:\n",
    "            invalidate_token(token)\n",
    "    else:\n",
    "        print(f\"Token {token} not found in the list.\")\n",
    "\n",
    "# Function to make a request to the API\n",
    "def request (path):\n",
    "    # Set the URL to the API endpoint\n",
    "    url = f'{API_URL}/{path}'\n",
    "    if (path.startswith(API_URL)):\n",
    "        url = path\n",
    "\n",
    "    # Define headers with the token\n",
    "    token = get_avaliable_token()\n",
    "    headers = {}\n",
    "    if token is not None:\n",
    "        headers['Authorization'] = f'Bearer {token}'\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.ok:\n",
    "        # print(f\"Request successful for token: {token}\")\n",
    "        used_token(token)\n",
    "    else:\n",
    "        if response.status_code == 401:\n",
    "            print(f\"Unauthorized access for token: {token}\")\n",
    "            used_token(token)\n",
    "\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"Rate limit exceeded for token: {token}\")\n",
    "            invalidate_token(token)\n",
    "            # Retry with the next available token\n",
    "            if (token is not None):\n",
    "                print(\"Retrying with the next available token...\")\n",
    "                return request(path)\n",
    "            \n",
    "        elif response.status_code == 404:\n",
    "            print(\"Resource not found\")\n",
    "\n",
    "        elif response.status_code == 429:\n",
    "            print(\"Rate limit exceeded for IP\")\n",
    "            invalidate_token(token)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and form/save initial graph\n",
    "\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "SEED_USER = 'gabriel-dp' # Initial node\n",
    "MAX_REQUESTS = 15000 # Max Requests (generally keep 5k per token)\n",
    "MAX_DEPTH = 3 # How deep will we search\n",
    "BLOCKED_USERS = {'torvalds'} # Can desconsider some users\n",
    "SLEEP_TIME = 0.2  # Delay between requests\n",
    "\n",
    "# Cache for using less requests\n",
    "followers_cache = {}\n",
    "following_cache = {}\n",
    "\n",
    "# Fetch followers/following (Can limit number of pages)\n",
    "def get_all_follow_data(username, follow_type='followers'):\n",
    "    print(f\"getting all follow data for {username}\")\n",
    "    if follow_type == 'followers' and username in followers_cache:\n",
    "        return followers_cache[username]\n",
    "    if follow_type == 'following' and username in following_cache:\n",
    "        return following_cache[username]\n",
    "\n",
    "    per_page = 100\n",
    "    page = 1\n",
    "    pages_limit = 10\n",
    "    results = []\n",
    "\n",
    "    while True:\n",
    "        path = f'users/{username}/{follow_type}?per_page={per_page}&page={page}'\n",
    "        response = request(path)\n",
    "        if not response or not response.ok:\n",
    "            break\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            break\n",
    "        results.extend([user['login'] for user in data])\n",
    "        if len(data) < per_page or page >= pages_limit:\n",
    "            break\n",
    "        page += 1\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "    if follow_type == 'followers':\n",
    "        followers_cache[username] = results\n",
    "    else:\n",
    "        following_cache[username] = results\n",
    "\n",
    "    return results\n",
    "\n",
    "# Build graph (mutual)\n",
    "def build_mutual_graph(seed_user, max_requests=10000, max_depth=3):\n",
    "    G = nx.Graph()\n",
    "    visited = set()\n",
    "    queue = [(seed_user, 0)]\n",
    "    total_requests = 0\n",
    "\n",
    "    while queue and total_requests < max_requests:\n",
    "        user, level = queue.pop(0)\n",
    "\n",
    "        if user in visited or user in BLOCKED_USERS or level > max_depth:\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing {user} (level {level}) | Total requests: {total_requests}\")\n",
    "        visited.add(user)\n",
    "\n",
    "        try:\n",
    "            followers = get_all_follow_data(user, 'followers')\n",
    "            total_requests += 1\n",
    "            following = get_all_follow_data(user, 'following')\n",
    "            total_requests += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for {user}: {e}\")\n",
    "            continue\n",
    "\n",
    "        mutuals = set(followers).intersection(following)\n",
    "        for mutual in mutuals:\n",
    "            if mutual in BLOCKED_USERS:\n",
    "                continue\n",
    "            G.add_edge(user, mutual)\n",
    "\n",
    "            if mutual not in visited:\n",
    "                queue.append((mutual, level + 1))\n",
    "\n",
    "        time.sleep(SLEEP_TIME)\n",
    "\n",
    "    print(f\"Finished: {len(G.nodes())} nodes, {len(G.edges())} edges, {total_requests} requests\")\n",
    "    return G\n",
    "\n",
    "# Save graph\n",
    "def save_graph(graph, filename='github_mutual_follow_graph.graphml'):\n",
    "    nx.write_graphml(graph, filename)\n",
    "    print(f\"Graph saved to {filename}\")\n",
    "\n",
    "# Entry piont\n",
    "if __name__ == \"__main__\":\n",
    "    graph = build_mutual_graph(SEED_USER, MAX_REQUESTS, MAX_DEPTH)\n",
    "    save_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dddab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and print the caracterization of the network\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load graph from file\n",
    "filename = 'github_mutual_follow_graph.graphml'\n",
    "G_original = nx.read_graphml(filename)\n",
    "mutual_edges = [(u, v) for u, v in G_original.edges() if G_original.has_edge(v, u)]\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(mutual_edges)\n",
    "\n",
    "print(\"Número de vértices:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())\n",
    "\n",
    "# Plot degree distribution\n",
    "degree_sequence = [d for n, d in G.degree()]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(degree_sequence, bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribuição de graus\")\n",
    "plt.xlabel(\"Grau\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate clustering\n",
    "clustering = nx.clustering(G)\n",
    "avg_clustering = np.mean(list(clustering.values()))\n",
    "print(\"Coeficiente de clustering médio:\", avg_clustering)\n",
    "\n",
    "full_clustering_nodes = [n for n, c in clustering.items() if c == 1.0]\n",
    "print(\"Número de nós com clustering = 1.0:\", len(full_clustering_nodes))\n",
    "\n",
    "# Plotting degrees of clusterings of 1.0\n",
    "sizes = [G.degree(n) for n in full_clustering_nodes]\n",
    "print(\"Graus médios dos nós com clustering = 1.0:\", np.mean(sizes))\n",
    "plt.hist(sizes, bins=20, color='orange', edgecolor='black')\n",
    "plt.title(\"Distribuição de grau dos nós com clustering = 1.0\")\n",
    "plt.xlabel(\"Grau\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.show()\n",
    "\n",
    "# Calculates centralities (degree and eigenvector)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "try:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "except nx.PowerIterationFailedConvergence:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "print(\"eigenvector centrality done\")\n",
    "\n",
    "# Top 5 by degree centrality\n",
    "top_5_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 usuários por centralidade de grau:\")\n",
    "for node, centrality in top_5_degree:\n",
    "    print(f\"Nó: {node}, Centralidade de grau: {centrality:.4f}\")\n",
    "\n",
    "# Top 5 by eigenvector centrality\n",
    "top_5_eigen = sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"\\nTop 5 usuários por centralidade de autovetor:\")\n",
    "for node, centrality in top_5_eigen:\n",
    "    print(f\"Nó: {node}, Centralidade de autovetor: {centrality:.4f}\")\n",
    "\n",
    "# Remove edges with lower than 1 degree for performance and better visualization\n",
    "low_degree_nodes = [n for n, d in G.degree() if d <= 5]\n",
    "G.remove_nodes_from(low_degree_nodes)\n",
    "print(\"Número de vértices:\", G.number_of_nodes())\n",
    "print(\"Número de arestas:\", G.number_of_edges())\n",
    "\n",
    "\n",
    "# Scaling node size\n",
    "scaler = MinMaxScaler(feature_range=(5, 150))\n",
    "deg_vals = np.array([degree_centrality[n] for n in G.nodes()]).reshape(-1, 1)\n",
    "eig_vals = np.array([eigenvector_centrality[n] for n in G.nodes()]).reshape(-1, 1)\n",
    "degree_sizes = scaler.fit_transform(deg_vals).flatten()\n",
    "eigen_sizes = scaler.fit_transform(eig_vals).flatten()\n",
    "print(\"scaling done\")\n",
    "\n",
    "# Layout (kamada_kawai or spring)\n",
    "pos = nx.spring_layout(G, k=1.5, iterations=200, seed=42)\n",
    "print(\"positioning done\")\n",
    "\n",
    "# Degree plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=degree_sizes, alpha=0.8, node_color='teal')\n",
    "print(\"drawn nodes\")\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.05, edge_color='gray', width=0.3)\n",
    "print(\"drawn edges\")\n",
    "plt.title(\"Tamanhos proporcionais à centralidade de grau\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Eigenvector plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=eigen_sizes, alpha=0.8, node_color='royalblue')\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.05, edge_color='gray', width=0.3)\n",
    "plt.title(\"Tamanhos proporcionais à centralidade de autovetor\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting subgraphs\n",
    "\n",
    "k = 300\n",
    "# Top k degree graph\n",
    "top_nodes_degree = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:k]\n",
    "G_top_degree = G.subgraph(top_nodes_degree)\n",
    "\n",
    "# Top K eigenvector graph\n",
    "top_nodes_eigenvector = sorted(eigenvector_centrality, key=eigenvector_centrality.get, reverse=True)[:k]\n",
    "G_top_eigenvector = G.subgraph(top_nodes_eigenvector)\n",
    "\n",
    "def scale_sizes(subgraph, centrality_dict):\n",
    "    scaler = MinMaxScaler(feature_range=(5, 150))\n",
    "    centrality_vals = np.array([centrality_dict[n] for n in subgraph.nodes()]).reshape(-1, 1)\n",
    "    scaled_sizes = scaler.fit_transform(centrality_vals).flatten()\n",
    "    return scaled_sizes\n",
    "\n",
    "# degree plot\n",
    "degree_sizes_sub = scale_sizes(G_top_degree, degree_centrality)\n",
    "# pos_degree = nx.kamada_kawai_layout(G_top_degree)\n",
    "pos_degree = nx.spring_layout(G_top_degree, k=0.5, iterations=200, seed=42)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G_top_degree, pos_degree, node_size=degree_sizes_sub, alpha=0.8, node_color='teal')\n",
    "nx.draw_networkx_edges(G_top_degree, pos_degree, alpha=0.1, edge_color='gray', width=0.4)\n",
    "plt.title(f\"Top {k} Nós - Centralidade de Grau\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# eigenvector plot\n",
    "eigen_sizes_sub = scale_sizes(G_top_eigenvector, eigenvector_centrality)\n",
    "# pos_eigen = nx.kamada_kawai_layout(G_top_eigenvector)\n",
    "pos_eigen = nx.spring_layout(G_top_eigenvector, k=0.5, iterations=200, seed=42)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "nx.draw_networkx_nodes(G_top_eigenvector, pos_eigen, node_size=eigen_sizes_sub, alpha=0.8, node_color='royalblue')\n",
    "nx.draw_networkx_edges(G_top_eigenvector, pos_eigen, alpha=0.1, edge_color='gray', width=0.4)\n",
    "plt.title(f\"Top {k} Nós - Centralidade de Autovetor\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8098dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting data from users in our network\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "G = nx.read_graphml(\"github_mutual_follow_graph.graphml\")\n",
    "top_users = sorted(G.degree, key=lambda x: x[1], reverse=True)[15001:17500]\n",
    "top_usernames = [user for user, _ in top_users]\n",
    "\n",
    "collected_data = {}\n",
    "i=1\n",
    "for username in top_usernames:\n",
    "    print(i)\n",
    "    i+=1\n",
    "    try:\n",
    "        print(f\"Fetching data for: {username}\")\n",
    "        user_response = request(f\"users/{username}\")\n",
    "        repos_response = request(f\"users/{username}/repos?per_page=100\")\n",
    "        user_data = user_response.json()\n",
    "        repos_data = repos_response.json()\n",
    "        collected_data[username] = {\n",
    "            \"login\": user_data.get(\"login\"),\n",
    "            \"name\": user_data.get(\"name\"),\n",
    "            \"followers\": user_data.get(\"followers\"),\n",
    "            \"following\": user_data.get(\"following\"),\n",
    "            \"location\": user_data.get(\"location\"),\n",
    "            \"created_at\": user_data.get(\"created_at\"),\n",
    "            \"company\": user_data.get(\"company\"),\n",
    "            \"bio\": user_data.get(\"bio\"),\n",
    "            \"public_repos\": user_data.get(\"public_repos\"),\n",
    "            \"repos\": repos_data\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {username}: {e}\")\n",
    "\n",
    "with open(\"github_user_data_15000_17500.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(collected_data, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the Data into tabular numerical attributes\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Load all JSON files matching the pattern\n",
    "all_files = [f for f in os.listdir() if f.startswith(\"github_user_data_\") and f.endswith(\".json\")]\n",
    "raw_data = {}\n",
    "\n",
    "print(len(all_files))\n",
    "\n",
    "for file in all_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        raw_data.update(data)\n",
    "\n",
    "print(f\"Total users loaded: {len(raw_data)}\")\n",
    "\n",
    "language_user_counts = Counter()\n",
    "temp_processed_data = []\n",
    "\n",
    "for username, user in raw_data.items():\n",
    "    try:\n",
    "        repos = user.get(\"repos\", [])\n",
    "        created_at = user.get(\"created_at\")\n",
    "\n",
    "        if not created_at:\n",
    "            continue\n",
    "\n",
    "        created_year = datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%SZ\").year\n",
    "        time_in_years = 2025 - created_year\n",
    "\n",
    "        langs = [repo[\"language\"] for repo in repos if repo.get(\"language\")]\n",
    "        lang_counter = Counter(langs)\n",
    "\n",
    "        # Count how many users used each language\n",
    "        for lang in lang_counter:\n",
    "            language_user_counts[lang.lower()] += 1\n",
    "\n",
    "        total_stars = sum(repo.get(\"stargazers_count\", 0) for repo in repos)\n",
    "        average_stars = total_stars / len(repos) if repos else 0\n",
    "\n",
    "        user_row = {\n",
    "            \"login\": user.get(\"login\"),\n",
    "            \"followers\": user.get(\"followers\"),\n",
    "            \"following\": user.get(\"following\"),\n",
    "            \"company\": int(user.get(\"company\") is not None),\n",
    "            \"has_bio\": int(user.get(\"bio\") is not None),\n",
    "            \"time_in_years\": time_in_years,\n",
    "            \"public_repos\": user.get(\"public_repos\"),\n",
    "            \"average_stars\": round(average_stars, 2),\n",
    "            \"languages_used\": len(lang_counter)\n",
    "        }\n",
    "\n",
    "        user_row[\"_lang_counter\"] = {lang.lower(): count for lang, count in lang_counter.items()}\n",
    "        temp_processed_data.append(user_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {username}: {e}\")\n",
    "\n",
    "# Keep only languages used by at least 50 users\n",
    "valid_languages = {lang for lang, count in language_user_counts.items() if count >= 50}\n",
    "\n",
    "processed_data = []\n",
    "for user in temp_processed_data:\n",
    "    lang_counts = user.pop(\"_lang_counter\")\n",
    "    for lang in valid_languages:\n",
    "        user[lang] = lang_counts.get(lang, 0)\n",
    "    processed_data.append(user)\n",
    "\n",
    "# Write to CSV\n",
    "base_fields = [\n",
    "    \"login\", \"followers\", \"following\",\n",
    "    \"company\", \"has_bio\", \"time_in_years\",\n",
    "    \"public_repos\", \"average_stars\", \"languages_used\"\n",
    "]\n",
    "language_fields = sorted(valid_languages)\n",
    "fieldnames = base_fields + language_fields\n",
    "\n",
    "output_file = \"github_user_tab_data.csv\"\n",
    "with open(output_file, \"w\", newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(processed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c34657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Stats and Plots for the base attributes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"github_user_tab_data.csv\")\n",
    "\n",
    "base_fields = [\n",
    "    \"followers\", \"following\", \"company\", \"has_bio\",\n",
    "    \"time_in_years\", \"public_repos\", \"average_stars\", \"languages_used\"\n",
    "]\n",
    "\n",
    "os.makedirs(\"docs2\", exist_ok=True)\n",
    "\n",
    "stats = df[base_fields].describe().transpose()\n",
    "stats[\"median\"] = df[base_fields].median()\n",
    "stats = stats[[\"min\", \"max\", \"mean\", \"median\", \"std\"]]\n",
    "stats.to_csv(\"docs2/github_user_base_stats.csv\")\n",
    "\n",
    "for col in base_fields:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df[col], bins=30, kde=True)\n",
    "    plt.title(f\"Histogram of {col}\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f\"Boxplot of {col}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"docs2/{col}_distribution.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ff0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap for our base fields\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "os.makedirs(\"docs2\", exist_ok=True)\n",
    "\n",
    "base_fields = [\n",
    "    \"followers\", \"following\",\n",
    "    \"company\", \"has_bio\", \"time_in_years\",\n",
    "    \"public_repos\", \"average_stars\", \"languages_used\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"github_user_tab_data.csv\")\n",
    "df_base = df[base_fields].copy()\n",
    "\n",
    "corr = df_base.corr()\n",
    "corr.to_csv(\"docs2/correlation_matrix.csv\")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Correlation Heatmap of Base Fields\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"docs2/correlation_heatmap.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2033e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the values from 0 to 1\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "base_fields = [\n",
    "    \"followers\", \"following\",\n",
    "    \"company\", \"has_bio\", \"time_in_years\",\n",
    "    \"public_repos\", \"average_stars\", \"languages_used\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"github_user_tab_data.csv\")\n",
    "login_col = df[\"login\"]\n",
    "df_to_scale = df[base_fields]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_to_scale), columns=base_fields)\n",
    "\n",
    "df_final = pd.concat([login_col, df_scaled], axis=1)\n",
    "df_final.to_csv(\"github_user_tab_data_normalized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ef361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to choose best K\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"github_user_tab_data_normalized.csv\")\n",
    "X = df.drop(columns=[\"login\"])\n",
    "\n",
    "inertias = []\n",
    "k_values = range(2, 11)\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs2/kmeans_elbow_plot.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80319f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering data and visualizing it\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"github_user_tab_data_normalized.csv\")\n",
    "\n",
    "feature_columns = df.columns.drop('login')\n",
    "X = df[feature_columns]\n",
    "\n",
    "k = 4 #choosen through elbow method\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "df['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "df[['login', 'cluster']].to_csv('user_clusters.csv', index=False)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_score(X, df['cluster'])}\")\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=components[:, 0], y=components[:, 1], hue=df['cluster'], palette='tab10', s=30)\n",
    "plt.title('User Clusters')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('docs2/user_clusters_pca.png')\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from math import comb\n",
    "\n",
    "G = nx.read_graphml(\"github_mutual_follow_graph.graphml\")\n",
    "\n",
    "df_clusters = pd.read_csv(\"user_clusters.csv\")\n",
    "user_cluster = dict(zip(df_clusters['login'], df_clusters['cluster']))\n",
    "\n",
    "cluster_edge_counts = defaultdict(int)\n",
    "cluster_possible_edges = defaultdict(int)\n",
    "total_edges = G.number_of_edges()\n",
    "\n",
    "for u, v in G.edges():\n",
    "    cluster_u = user_cluster.get(u)\n",
    "    cluster_v = user_cluster.get(v)\n",
    "    if cluster_u is not None and cluster_v is not None:\n",
    "        if cluster_u == cluster_v:\n",
    "            cluster_edge_counts[cluster_u] += 1\n",
    "\n",
    "k = df_clusters['cluster'].nunique()\n",
    "\n",
    "for cluster in range(k):\n",
    "    nodes_in_cluster = [node for node, c in user_cluster.items() if c == cluster]\n",
    "    n = len(nodes_in_cluster)\n",
    "    cluster_possible_edges[cluster] = comb(n, 2) if n >= 2 else 0\n",
    "\n",
    "print(\"\\nIntra-cluster edge density:\")\n",
    "for cluster in range(k):\n",
    "    edges = cluster_edge_counts[cluster]\n",
    "    possible = cluster_possible_edges[cluster]\n",
    "    density = edges / possible if possible > 0 else 0\n",
    "    print(f\"Cluster {cluster}: {edges} edges out of {possible} possible (density={density:.4f})\")\n",
    "\n",
    "graph_density = total_edges / comb(G.number_of_nodes(), 2)\n",
    "print(f\"\\nGraph-wide edge density: {graph_density:.4f}\")\n",
    "\n",
    "for cluster in range(k):\n",
    "    edges = cluster_edge_counts[cluster]\n",
    "    possible = cluster_possible_edges[cluster]\n",
    "    density = edges / possible if possible > 0 else 0\n",
    "    density_ratio = density / graph_density if graph_density > 0 else 0\n",
    "    print(f\"{cluster}\\t{edges}\\t{possible}\\t{density:.6f}\\t{density_ratio:.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
